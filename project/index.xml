<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Projects on Christina Tsangouri</title>
    <link>https://christinatsan.github.io/project/</link>
    <description>Recent content in Projects on Christina Tsangouri</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018</copyright>
    <lastBuildDate>Wed, 27 Apr 2016 00:00:00 +0000</lastBuildDate>
    <atom:link href="/project/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>EmoTrain</title>
      <link>https://christinatsan.github.io/project/emotrain/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>https://christinatsan.github.io/project/emotrain/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://christinatsan.github.io/img/emotrain_interface2.png&#34; alt=&#34;&#34; /&gt;
&lt;img src=&#34;https://christinatsan.github.io/img/emotrain_interface3.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Emotions  are  an  incredibly  important  aspect  of human   communication.   Individuals   with   Autism   Spectrum Disorder  (ASD)  suffer  from  significant  challenges  in  perceiving, and understanding others’emotions, and responding emotionally in an appropriate manner. In order to help themto improve their facial  expression  recognition  and  response  skills,  EmoTrain  was developedasan interactive platform that utilizes deep learning to track  a  user’s  facial  expressions  from  a  mobile  device’s camera and  assists  a  userin  learning  to  understand  facial  expressions and  how  to  perform  them.The paperdetails  the  design  of  the EmoTrain  platform,  the  deep  learning  model  and  game  engine for the platform, and evaluates EmoTrain’s effectiveness.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Hybrid BLE-Tango Indoor Positioning</title>
      <link>https://christinatsan.github.io/project/hybrid/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>https://christinatsan.github.io/project/hybrid/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://christinatsan.github.io/img/assistapp.png&#34; alt=&#34;&#34; /&gt;
&lt;img src=&#34;https://christinatsan.github.io/img/paths.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Blind &amp;amp; visually impaired individuals often face challenges in wayfinding in unfamiliar environments. Thus, an accessible indoor positioning and navigation system that safely and accurately positions and guides such individuals would be welcome. In indoor positioning, both Bluetooth Low Energy (BLE) beacons and Google Tango have their individual strengths but also have weaknesses that can affect the overall usability of a system that solely relies on either component. We propose a hybrid positioning and navigation system that combines both BLE beacons and Google Tango in order to tap into their strengths while minimizing their individual weaknesses.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Real-time Facial Computing App for the Visually Impaired</title>
      <link>https://christinatsan.github.io/project/emo-vip/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>https://christinatsan.github.io/project/emo-vip/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://christinatsan.github.io/img/emovip1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The real-time facial computing project focuses on the design and implementation of an emotion-computing framework in order to help visually impaired people to be able to recognize the emotions of the people around them.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>SmartCane</title>
      <link>https://christinatsan.github.io/project/smartcane/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>https://christinatsan.github.io/project/smartcane/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://christinatsan.github.io/img/user_smartcane.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The   CCNY SmartCane  system is a  robotic  white  cane  and  mobile  device navigation  software  for  visually  impaired  people.  The  system includes   software   for   Google   Tango   devices   that   utilizes simultaneous  localization  and  mapping  (SLAM)  to  plan  a  path and  guide  a  visually  impaired  user  to  waypoints  within  indoor environments. A control panel is mounted on the standard white cane  that  enables  visually  impaired  users  to  communicate  with the   navigation   software   and   is   additionally   used   to   provide navigation instructions via haptic feedback. Based on the motion-tracking  and  localization  capabilities  of  the  Google  Tango,  the SmartCane  is  able to  generate  a  safe  path  to  the  destination waypoint indicated by the user.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
